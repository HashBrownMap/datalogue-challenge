{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "data = pd.read_csv('./data/data.csv', sep=',', dtype=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>response_id</th>\n",
       "      <th>class</th>\n",
       "      <th>response_text</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "      <th>Unnamed: 5</th>\n",
       "      <th>Unnamed: 6</th>\n",
       "      <th>Unnamed: 7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>response_1</td>\n",
       "      <td>not_flagged</td>\n",
       "      <td>I try and avoid this sort of conflict</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>response_2</td>\n",
       "      <td>flagged</td>\n",
       "      <td>Had a friend open up to me about his mental ad...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>response_3</td>\n",
       "      <td>flagged</td>\n",
       "      <td>I saved a girl from suicide once. She was goin...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>response_4</td>\n",
       "      <td>not_flagged</td>\n",
       "      <td>i cant think of one really...i think i may hav...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>response_5</td>\n",
       "      <td>not_flagged</td>\n",
       "      <td>Only really one friend who doesn't fit into th...</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  response_id        class                                      response_text  \\\n",
       "0  response_1  not_flagged              I try and avoid this sort of conflict   \n",
       "1  response_2      flagged  Had a friend open up to me about his mental ad...   \n",
       "2  response_3      flagged  I saved a girl from suicide once. She was goin...   \n",
       "3  response_4  not_flagged  i cant think of one really...i think i may hav...   \n",
       "4  response_5  not_flagged  Only really one friend who doesn't fit into th...   \n",
       "\n",
       "  Unnamed: 3  Unnamed: 4 Unnamed: 5  Unnamed: 6 Unnamed: 7  \n",
       "0        NaN         NaN        NaN         NaN        NaN  \n",
       "1        NaN         NaN        NaN         NaN        NaN  \n",
       "2        NaN         NaN        NaN         NaN        NaN  \n",
       "3        NaN         NaN        NaN         NaN        NaN  \n",
       "4                    NaN        NaN         NaN        NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last five columns are not valuable so let's get rid of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cols = [0,3,4,5,6,7]\n",
    "data.drop(data.columns[cols], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's now replace the class to 0 and 1s (flagged=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['class'].replace('not_flagged', 0, inplace=True)\n",
    "data['class'].replace('flagged', 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>response_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>I try and avoid this sort of conflict</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Had a friend open up to me about his mental ad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>I saved a girl from suicide once. She was goin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>i cant think of one really...i think i may hav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Only really one friend who doesn't fit into th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   class                                      response_text\n",
       "0      0              I try and avoid this sort of conflict\n",
       "1      1  Had a friend open up to me about his mental ad...\n",
       "2      1  I saved a girl from suicide once. She was goin...\n",
       "3      0  i cant think of one really...i think i may hav...\n",
       "4      0  Only really one friend who doesn't fit into th..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's see the distribution of the class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    55\n",
       "1    25\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['class'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing and Visualization of Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we will first lower case each word, remove punctuations and remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>response_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>try avoid sort conflict</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>friend open mental addiction weed taking life ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>saved girl suicide going swallow bunch pills t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>cant think one reallyi think may indirectly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>really one friend doesnt fit categories therap...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   class                                      response_text\n",
       "0      0                            try avoid sort conflict\n",
       "1      1  friend open mental addiction weed taking life ...\n",
       "2      1  saved girl suicide going swallow bunch pills t...\n",
       "3      0        cant think one reallyi think may indirectly\n",
       "4      0  really one friend doesnt fit categories therap..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Removing punctuations\n",
    "def removePunctuation(s):\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    return s.translate(translator)\n",
    "\n",
    "# Lowercasing words and removing stopwords\n",
    "def lowerCaseStopWords(s):\n",
    "    sw = stopwords.words('english')\n",
    "    np.array(sw)\n",
    "    \n",
    "    s = [word.lower() for word in s.split() if word.lower() not in sw]\n",
    "    return \" \".join(s)\n",
    "\n",
    "data['response_text'] = data['response_text'].apply(removePunctuation)\n",
    "data['response_text'] = data['response_text'].apply(lowerCaseStopWords)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "let's now tokenize each response_text, stem each example and look at the length of our examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>response_text</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>try avoid sort conflict</td>\n",
       "      <td>[try, avoid, sort, conflict]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>friend open mental addiction weed taking life ...</td>\n",
       "      <td>[friend, open, mental, addiction, weed, taking...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>saved girl suicide going swallow bunch pills t...</td>\n",
       "      <td>[saved, girl, suicide, going, swallow, bunch, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>cant think one reallyi think may indirectly</td>\n",
       "      <td>[cant, think, one, reallyi, think, may, indire...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>really one friend doesnt fit categories therap...</td>\n",
       "      <td>[really, one, friend, doesnt, fit, categories,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   class                                      response_text  \\\n",
       "0      0                            try avoid sort conflict   \n",
       "1      1  friend open mental addiction weed taking life ...   \n",
       "2      1  saved girl suicide going swallow bunch pills t...   \n",
       "3      0        cant think one reallyi think may indirectly   \n",
       "4      0  really one friend doesnt fit categories therap...   \n",
       "\n",
       "                                              tokens  \n",
       "0                       [try, avoid, sort, conflict]  \n",
       "1  [friend, open, mental, addiction, weed, taking...  \n",
       "2  [saved, girl, suicide, going, swallow, bunch, ...  \n",
       "3  [cant, think, one, reallyi, think, may, indire...  \n",
       "4  [really, one, friend, doesnt, fit, categories,...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "rxtokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "data['tokens'] = data['response_text'].apply(rxtokenizer.tokenize)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets now use a lemmatize to represent related words by their base string. lemmatize is important with the small dataset we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>response_text</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[try, avoid, sort, conflict]</td>\n",
       "      <td>[try, avoid, sort, conflict]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[friend, open, mental, addiction, wee, take, l...</td>\n",
       "      <td>[friend, open, mental, addiction, wee, take, l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>[save, girl, suicide, go, swallow, bunch, pill...</td>\n",
       "      <td>[save, girl, suicide, go, swallow, bunch, pill...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>[cant, think, one, reallyi, think, may, indire...</td>\n",
       "      <td>[cant, think, one, reallyi, think, may, indire...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>[really, one, friend, doesnt, fit, category, t...</td>\n",
       "      <td>[really, one, friend, doesnt, fit, category, t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   class                                      response_text  \\\n",
       "0      0                       [try, avoid, sort, conflict]   \n",
       "1      1  [friend, open, mental, addiction, wee, take, l...   \n",
       "2      1  [save, girl, suicide, go, swallow, bunch, pill...   \n",
       "3      0  [cant, think, one, reallyi, think, may, indire...   \n",
       "4      0  [really, one, friend, doesnt, fit, category, t...   \n",
       "\n",
       "                                              tokens  \n",
       "0                       [try, avoid, sort, conflict]  \n",
       "1  [friend, open, mental, addiction, wee, take, l...  \n",
       "2  [save, girl, suicide, go, swallow, bunch, pill...  \n",
       "3  [cant, think, one, reallyi, think, may, indire...  \n",
       "4  [really, one, friend, doesnt, fit, category, t...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spacy.lang.en.lemmatizer import LOOKUP\n",
    "from spacy.lang.en import English\n",
    "\n",
    "def get_lemma(text):\n",
    "    return LOOKUP.get(text, text) # if no lemma found, return original text\n",
    "\n",
    "def lemmatize(tokens):\n",
    "    lemmatized_tokens = []\n",
    "    for w in tokens:\n",
    "        lemmatized_tokens.append(get_lemma(w))\n",
    "    return lemmatized_tokens\n",
    "def lemmatize_string(response):\n",
    "    lemmatized_response = []\n",
    "    for w in response.split(' '):\n",
    "        new_w = get_lemma(w)\n",
    "        lemmatized_response.append(new_w)\n",
    "    return lemmatized_response\n",
    "\n",
    "data['tokens'] = data['tokens'].apply(lemmatize)\n",
    "data['response_text'] = data['response_text'].apply(lemmatize_string)\n",
    "        \n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 474\n"
     ]
    }
   ],
   "source": [
    "all_words = [w for tokens in data['tokens'] for w in tokens]\n",
    "word_count = nltk.FreqDist(all_words)\n",
    "vocab = list(set(all_words))\n",
    "lengths = [len(tokens) for tokens in data['tokens']]\n",
    "\n",
    "print(\"Vocabulary size: %s\" % len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtEAAAJQCAYAAABIJTh6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhmv/Ul/RKvkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1228b80f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(figsize=(12,10))\n",
    "plt.hist(lengths)\n",
    "plt.xlabel(\"Length of sentence\")\n",
    "plt.ylabel(\"# of senstences\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's take a look at the longest response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['friend', 'would', 'go', 'girl', 'get', 'super', 'depress', 'tell', 'go', 'kill', 'one', 'night', 'drive', '1n', 'hour', 'half', 'go', 'look', 'cop', 'already', 'take', 'hospital', 'friend', 'go', 'hang', 'also', 'help', 'try', 'talk', 'another', 'person', 'hadnt', 'friend', 'year', 'cause', 'stuff', 'call', 'blue', 'thankgiving', 'last', 'year', 'call', 'back', 'get', 'hospital', 'numb', 'know', 'something', 'end', 'kid', 'friend', 'year', 'huge', 'douche', 'psych', 'ward', 'try', 'kill', 'get', 'hold', 'talk', 'whenever', 'would', 'call', 'would', 'answer', 'even', 'though', 'still', 'shit', 'year', 'early', 'mind', 'people', 'may', 'want', 'around', 'situation', 'like', 'ill', 'people', 'goal', 'purpose', 'life', 'thats', 'sustain', 'survival', 'help', 'make', 'people', 'live', 'little', 'bite', 'good', 'make', 'people', 'laugh', 'theyre', 'sad', 'help', 'people', 'nobody', 'else', 'basically', 'treat', 'human', 'even', 'strange', 'respect', 'kindness', 'friend', 'thats', 'people', 'need', 'sometime', 'let', 'people', 'talk', 'shit', 'offer', 'helpful', 'advice', 'im', 'still', 'swim', 'shit', 'thats', 'enough', 'keep', 'go', 'keep', 'little', 'light', 'end', 'tunnel', 'switch', 'idk', 'long', 'last', 'ill', 'till', 'flicker']\n"
     ]
    }
   ],
   "source": [
    "print(max(data['tokens'], key=len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('friend', 51), ('help', 33), ('talk', 25), ('go', 24), ('people', 22), ('good', 19), ('try', 14), ('get', 13), ('listen', 12), ('think', 11), ('feel', 11), ('one', 9), ('would', 9), ('problem', 9), ('need', 9)]\n"
     ]
    }
   ],
   "source": [
    "print(word_count.most_common(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now let's try to see the most common words grouped by sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('friend', 24), ('go', 14), ('help', 14), ('talk', 11), ('people', 11), ('get', 9), ('good', 9), ('try', 8), ('depression', 7), ('year', 6), ('would', 6), ('addiction', 5), ('make', 5), ('anxiety', 5), ('school', 5), ('one', 5), ('life', 4), ('find', 4), ('issue', 4), ('think', 4), ('deal', 4), ('back', 4), ('kill', 4), ('use', 4), ('girl', 3), ('suicide', 3), ('way', 3), ('can', 3), ('say', 3), ('severe', 3)]\n"
     ]
    }
   ],
   "source": [
    "flagged_data = data.loc[data['class'] == 1]\n",
    "not_flagged_data = data.loc[data['class'] == 0]\n",
    "\n",
    "flagged_words = [w for tokens in flagged_data['tokens'] for w in tokens]\n",
    "flagged_count = nltk.FreqDist(flagged_words)\n",
    "\n",
    "not_flagged_words = [w for tokens in not_flagged_data['tokens'] for w in tokens]\n",
    "not_flagged_count = nltk.FreqDist(not_flagged_words)\n",
    "\n",
    "print(flagged_count.most_common(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('friend', 27), ('help', 19), ('talk', 14), ('people', 11), ('go', 10), ('good', 10), ('listen', 9), ('feel', 9), ('think', 7), ('problem', 7), ('know', 7), ('do', 7), ('try', 6), ('someone', 6), ('always', 6), ('need', 6), ('come', 6), ('lot', 6), ('girl', 5), ('im', 5), ('one', 4), ('really', 4), ('call', 4), ('time', 4), ('ive', 4), ('let', 4), ('open', 4), ('sometimes', 4), ('issue', 4), ('life', 4)]\n"
     ]
    }
   ],
   "source": [
    "print(not_flagged_count.most_common(30))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "because 80 samples won't enough for us, we will use wordnet from nltk to do some data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.to_pickle(\"./data/processeddata.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>response_text</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>try avoid sort conflict</td>\n",
       "      <td>[try, avoid, sort, conflict]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>friend open mental addiction weed taking life ...</td>\n",
       "      <td>[friend, open, mental, addiction, wee, take, l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>saved girl suicide going swallow bunch pills t...</td>\n",
       "      <td>[save, girl, suicide, go, swallow, bunch, pill...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>cant think one reallyi think may indirectly</td>\n",
       "      <td>[cant, think, one, reallyi, think, may, indire...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>really one friend doesnt fit categories therap...</td>\n",
       "      <td>[really, one, friend, doesnt, fit, category, t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   class                                      response_text  \\\n",
       "0      0                            try avoid sort conflict   \n",
       "1      1  friend open mental addiction weed taking life ...   \n",
       "2      1  saved girl suicide going swallow bunch pills t...   \n",
       "3      0        cant think one reallyi think may indirectly   \n",
       "4      0  really one friend doesnt fit categories therap...   \n",
       "\n",
       "                                              tokens  \n",
       "0                       [try, avoid, sort, conflict]  \n",
       "1  [friend, open, mental, addiction, wee, take, l...  \n",
       "2  [save, girl, suicide, go, swallow, bunch, pill...  \n",
       "3  [cant, think, one, reallyi, think, may, indire...  \n",
       "4  [really, one, friend, doesnt, fit, category, t...  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "data = pd.read_pickle(\"./data/processeddata.pickle\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>response_text</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>try avoid sort conflict</td>\n",
       "      <td>[try, avoid, sort, conflict]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>friend open mental addiction weed taking life ...</td>\n",
       "      <td>[friend, open, mental, alcoholism, wee, take, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>saved girl suicide going swallow bunch pills t...</td>\n",
       "      <td>[save, girl, suicidal, go, swallow, bunch, pil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>cant think one reallyi think may indirectly</td>\n",
       "      <td>[cant, think, one, reallyi, think, may, indire...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>really one friend doesnt fit categories therap...</td>\n",
       "      <td>[really, one, friend, doesnt, fit, category, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>couple years ago friends going switch school l...</td>\n",
       "      <td>[couple, year, ago, friend, go, switch, school...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>roommate going death loss gf anything get bedroom</td>\n",
       "      <td>[roommate, go, death, loss, gf, anything, get,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>ive couple friends could say friends quite sev...</td>\n",
       "      <td>[ive, couple, friend, can, say, friend, quite,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>listened someone talk relationship troubles of...</td>\n",
       "      <td>[listening, someone, talk, relationship, troub...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>always listen comforted sister lost virgity ni...</td>\n",
       "      <td>[always, listen, comfort, sister, lose, virgit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>took week work packed car picked friend verge ...</td>\n",
       "      <td>[take, week, work, pack, car, pick, friend, ve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>memorial anniversary friends father give suppo...</td>\n",
       "      <td>[memorial, anniversary, friend, father, give, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>anxious girlfriend always needs help</td>\n",
       "      <td>[anxious, girlfriend, never, need, helping]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>never</td>\n",
       "      <td>[never]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>mom</td>\n",
       "      <td>[mom]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>ex gf cuttersuicidal got issues slowly dying i...</td>\n",
       "      <td>[ex, gf, cuttersuicidal, get, issue, slowly, d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>helped advise friends faced circumstances simi...</td>\n",
       "      <td>[helping, advise, friend, face, circumstance, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>ive helped friends</td>\n",
       "      <td>[ive, helping, friend]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>friend girl talk tryin make feel better</td>\n",
       "      <td>[friend, girl, talk, tryin, make, feel, well]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>expressing concern openness friends dealing tr...</td>\n",
       "      <td>[express, concern, openness, friend, deal, tro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "      <td>listening girlfriends problems</td>\n",
       "      <td>[listening, girlfriend, problem]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>friend thinking suicide days managed talk</td>\n",
       "      <td>[friend, think, suicidal, day, manage, talk]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>gone depression anxiety understand struggles p...</td>\n",
       "      <td>[go, agitation, nervousness, understand, strug...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0</td>\n",
       "      <td>sometimes friends bring issues feel give good ...</td>\n",
       "      <td>[sometimes, friend, bring, issue, feel, give, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "      <td>never guess people dont see guy would go</td>\n",
       "      <td>[never, guess, people, do, see, guy, would, go]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1</td>\n",
       "      <td>last year best friend diagnosed anxiety disord...</td>\n",
       "      <td>[last, year, well, friend, diagnose, nervousne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1</td>\n",
       "      <td>cleaning friends campsite slightly helping liv...</td>\n",
       "      <td>[clean, friend, campsite, slightly, helping, l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "      <td>listen pretty damn well better apposed telling...</td>\n",
       "      <td>[listening, pretty, damn, good, well, appose, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1</td>\n",
       "      <td>helping friend dealing alcoholic mother</td>\n",
       "      <td>[helping, friend, deal, alcoholic, mother]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0</td>\n",
       "      <td>used tutor homeless men shelter help obtain ge...</td>\n",
       "      <td>[use, tutor, homeless, man, shelter, helping, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>never do judge people much listening good time...</td>\n",
       "      <td>[never, do, judge, people, much, listening, go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>never do judge people much listening good time...</td>\n",
       "      <td>[never, do, judge, people, much, listening, go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>grandmother go sever depression shortly diffic...</td>\n",
       "      <td>[grandmother, go, sever, agitation, shortly, d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>grandmother go sever depression shortly diffic...</td>\n",
       "      <td>[grandmother, go, sever, agitation, shortly, d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>grandmother go sever depression shortly diffic...</td>\n",
       "      <td>[grandmother, go, sever, agitation, shortly, d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>grandmother go sever depression shortly diffic...</td>\n",
       "      <td>[grandmother, go, sever, agitation, shortly, d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>grandmother go sever depression shortly diffic...</td>\n",
       "      <td>[grandmother, go, sever, agitation, shortly, d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>grandmother go sever agitation shortly difficu...</td>\n",
       "      <td>[grandmother, go, sever, agitation, shortly, d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>grandmother go sever agitation shortly difficu...</td>\n",
       "      <td>[grandmother, go, sever, agitation, shortly, d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>grandmother go sever agitation shortly difficu...</td>\n",
       "      <td>[grandmother, go, sever, agitation, shortly, d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>ive although im even id like im extremely clos...</td>\n",
       "      <td>[ive, although, im, even, id, like, im, extrem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>ive although im even id like im extremely clos...</td>\n",
       "      <td>[ive, although, im, even, id, like, im, extrem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>ive although im even id like im extremely clos...</td>\n",
       "      <td>[ive, although, im, even, id, like, im, extrem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>ive although im even id like im extremely clos...</td>\n",
       "      <td>[ive, although, im, even, id, like, im, extrem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>ive although im even id like im extremely clos...</td>\n",
       "      <td>[ive, although, im, even, id, like, im, extrem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>ive although im even id like im extremely clos...</td>\n",
       "      <td>[ive, although, im, even, id, like, im, extrem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>ive although im even id like im extremely clos...</td>\n",
       "      <td>[ive, although, im, even, id, like, im, extrem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>ive although im even id like im extremely clos...</td>\n",
       "      <td>[ive, although, im, even, id, like, im, extrem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>girlfriend use serious addiction trouble start...</td>\n",
       "      <td>[girlfriend, use, serious, alcoholism, trouble...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>girlfriend use serious alcoholism trouble star...</td>\n",
       "      <td>[girlfriend, use, serious, alcoholism, trouble...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>girlfriend use serious alcoholism trouble star...</td>\n",
       "      <td>[girlfriend, use, serious, alcoholism, trouble...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>girlfriend use serious alcoholism trouble star...</td>\n",
       "      <td>[girlfriend, use, serious, alcoholism, trouble...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>girlfriend use serious alcoholism trouble star...</td>\n",
       "      <td>[girlfriend, use, serious, alcoholism, trouble...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>girlfriend use serious alcoholism trouble star...</td>\n",
       "      <td>[girlfriend, use, serious, alcoholism, trouble...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>girlfriend use serious alcoholism trouble star...</td>\n",
       "      <td>[girlfriend, use, serious, alcoholism, trouble...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>girlfriend use serious alcoholism trouble star...</td>\n",
       "      <td>[girlfriend, use, serious, alcoholism, trouble...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>one person ever talk go thing us talk together...</td>\n",
       "      <td>[one, person, ever, talk, go, thing, us, talk,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>one person ever talk go thing us talk together...</td>\n",
       "      <td>[one, person, ever, talk, go, thing, us, talk,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>one person ever talk go thing us talk together...</td>\n",
       "      <td>[one, person, ever, talk, go, thing, us, talk,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>one person ever talk go thing us talk together...</td>\n",
       "      <td>[one, person, ever, talk, go, thing, us, talk,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>360 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   class                                      response_text  \\\n",
       "0      0                            try avoid sort conflict   \n",
       "1      1  friend open mental addiction weed taking life ...   \n",
       "2      1  saved girl suicide going swallow bunch pills t...   \n",
       "3      0        cant think one reallyi think may indirectly   \n",
       "4      0  really one friend doesnt fit categories therap...   \n",
       "5      0  couple years ago friends going switch school l...   \n",
       "6      1  roommate going death loss gf anything get bedroom   \n",
       "7      1  ive couple friends could say friends quite sev...   \n",
       "8      0  listened someone talk relationship troubles of...   \n",
       "9      1  always listen comforted sister lost virgity ni...   \n",
       "10     0  took week work packed car picked friend verge ...   \n",
       "11     1  memorial anniversary friends father give suppo...   \n",
       "12     0               anxious girlfriend always needs help   \n",
       "13     0                                              never   \n",
       "14     0                                                mom   \n",
       "15     1  ex gf cuttersuicidal got issues slowly dying i...   \n",
       "16     0  helped advise friends faced circumstances simi...   \n",
       "17     0                                 ive helped friends   \n",
       "18     0            friend girl talk tryin make feel better   \n",
       "19     0  expressing concern openness friends dealing tr...   \n",
       "20     0                     listening girlfriends problems   \n",
       "21     1          friend thinking suicide days managed talk   \n",
       "22     1  gone depression anxiety understand struggles p...   \n",
       "23     0  sometimes friends bring issues feel give good ...   \n",
       "24     0           never guess people dont see guy would go   \n",
       "25     1  last year best friend diagnosed anxiety disord...   \n",
       "26     1  cleaning friends campsite slightly helping liv...   \n",
       "27     0  listen pretty damn well better apposed telling...   \n",
       "28     1            helping friend dealing alcoholic mother   \n",
       "29     0  used tutor homeless men shelter help obtain ge...   \n",
       "..   ...                                                ...   \n",
       "0      0  never do judge people much listening good time...   \n",
       "0      0  never do judge people much listening good time...   \n",
       "0      1  grandmother go sever depression shortly diffic...   \n",
       "0      1  grandmother go sever depression shortly diffic...   \n",
       "0      1  grandmother go sever depression shortly diffic...   \n",
       "0      1  grandmother go sever depression shortly diffic...   \n",
       "0      1  grandmother go sever depression shortly diffic...   \n",
       "0      1  grandmother go sever agitation shortly difficu...   \n",
       "0      1  grandmother go sever agitation shortly difficu...   \n",
       "0      1  grandmother go sever agitation shortly difficu...   \n",
       "0      0  ive although im even id like im extremely clos...   \n",
       "0      0  ive although im even id like im extremely clos...   \n",
       "0      0  ive although im even id like im extremely clos...   \n",
       "0      0  ive although im even id like im extremely clos...   \n",
       "0      0  ive although im even id like im extremely clos...   \n",
       "0      0  ive although im even id like im extremely clos...   \n",
       "0      0  ive although im even id like im extremely clos...   \n",
       "0      0  ive although im even id like im extremely clos...   \n",
       "0      1  girlfriend use serious addiction trouble start...   \n",
       "0      1  girlfriend use serious alcoholism trouble star...   \n",
       "0      1  girlfriend use serious alcoholism trouble star...   \n",
       "0      1  girlfriend use serious alcoholism trouble star...   \n",
       "0      1  girlfriend use serious alcoholism trouble star...   \n",
       "0      1  girlfriend use serious alcoholism trouble star...   \n",
       "0      1  girlfriend use serious alcoholism trouble star...   \n",
       "0      1  girlfriend use serious alcoholism trouble star...   \n",
       "0      0  one person ever talk go thing us talk together...   \n",
       "0      0  one person ever talk go thing us talk together...   \n",
       "0      0  one person ever talk go thing us talk together...   \n",
       "0      0  one person ever talk go thing us talk together...   \n",
       "\n",
       "                                               tokens  \n",
       "0                        [try, avoid, sort, conflict]  \n",
       "1   [friend, open, mental, alcoholism, wee, take, ...  \n",
       "2   [save, girl, suicidal, go, swallow, bunch, pil...  \n",
       "3   [cant, think, one, reallyi, think, may, indire...  \n",
       "4   [really, one, friend, doesnt, fit, category, t...  \n",
       "5   [couple, year, ago, friend, go, switch, school...  \n",
       "6   [roommate, go, death, loss, gf, anything, get,...  \n",
       "7   [ive, couple, friend, can, say, friend, quite,...  \n",
       "8   [listening, someone, talk, relationship, troub...  \n",
       "9   [always, listen, comfort, sister, lose, virgit...  \n",
       "10  [take, week, work, pack, car, pick, friend, ve...  \n",
       "11  [memorial, anniversary, friend, father, give, ...  \n",
       "12        [anxious, girlfriend, never, need, helping]  \n",
       "13                                            [never]  \n",
       "14                                              [mom]  \n",
       "15  [ex, gf, cuttersuicidal, get, issue, slowly, d...  \n",
       "16  [helping, advise, friend, face, circumstance, ...  \n",
       "17                             [ive, helping, friend]  \n",
       "18      [friend, girl, talk, tryin, make, feel, well]  \n",
       "19  [express, concern, openness, friend, deal, tro...  \n",
       "20                   [listening, girlfriend, problem]  \n",
       "21       [friend, think, suicidal, day, manage, talk]  \n",
       "22  [go, agitation, nervousness, understand, strug...  \n",
       "23  [sometimes, friend, bring, issue, feel, give, ...  \n",
       "24    [never, guess, people, do, see, guy, would, go]  \n",
       "25  [last, year, well, friend, diagnose, nervousne...  \n",
       "26  [clean, friend, campsite, slightly, helping, l...  \n",
       "27  [listening, pretty, damn, good, well, appose, ...  \n",
       "28         [helping, friend, deal, alcoholic, mother]  \n",
       "29  [use, tutor, homeless, man, shelter, helping, ...  \n",
       "..                                                ...  \n",
       "0   [never, do, judge, people, much, listening, go...  \n",
       "0   [never, do, judge, people, much, listening, go...  \n",
       "0   [grandmother, go, sever, agitation, shortly, d...  \n",
       "0   [grandmother, go, sever, agitation, shortly, d...  \n",
       "0   [grandmother, go, sever, agitation, shortly, d...  \n",
       "0   [grandmother, go, sever, agitation, shortly, d...  \n",
       "0   [grandmother, go, sever, agitation, shortly, d...  \n",
       "0   [grandmother, go, sever, agitation, shortly, d...  \n",
       "0   [grandmother, go, sever, agitation, shortly, d...  \n",
       "0   [grandmother, go, sever, agitation, shortly, d...  \n",
       "0   [ive, although, im, even, id, like, im, extrem...  \n",
       "0   [ive, although, im, even, id, like, im, extrem...  \n",
       "0   [ive, although, im, even, id, like, im, extrem...  \n",
       "0   [ive, although, im, even, id, like, im, extrem...  \n",
       "0   [ive, although, im, even, id, like, im, extrem...  \n",
       "0   [ive, although, im, even, id, like, im, extrem...  \n",
       "0   [ive, although, im, even, id, like, im, extrem...  \n",
       "0   [ive, although, im, even, id, like, im, extrem...  \n",
       "0   [girlfriend, use, serious, alcoholism, trouble...  \n",
       "0   [girlfriend, use, serious, alcoholism, trouble...  \n",
       "0   [girlfriend, use, serious, alcoholism, trouble...  \n",
       "0   [girlfriend, use, serious, alcoholism, trouble...  \n",
       "0   [girlfriend, use, serious, alcoholism, trouble...  \n",
       "0   [girlfriend, use, serious, alcoholism, trouble...  \n",
       "0   [girlfriend, use, serious, alcoholism, trouble...  \n",
       "0   [girlfriend, use, serious, alcoholism, trouble...  \n",
       "0   [one, person, ever, talk, go, thing, us, talk,...  \n",
       "0   [one, person, ever, talk, go, thing, us, talk,...  \n",
       "0   [one, person, ever, talk, go, thing, us, talk,...  \n",
       "0   [one, person, ever, talk, go, thing, us, talk,...  \n",
       "\n",
       "[360 rows x 3 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_default', parser=False)\n",
    "\n",
    "d = {'class': [], 'response_text': [], 'tokens': []}\n",
    "generated = pd.DataFrame(columns=['class', 'response_text', 'tokens'])\n",
    "\n",
    "new_list = []\n",
    "\n",
    "negative_words = {'severe':[], 'kill':[],'depression':[], 'anxiety':[], 'drug':[], 'addiction':[], 'suicide':[], 'help':[]}\n",
    "positive_words = {'open':[], 'listen':[], 'life':[], 'always':[], 'help':[]}\n",
    "def get_related(w):\n",
    "    word = nlp.vocab[w]\n",
    "    filtered_words = [w for w in word.vocab if w.is_lower == word.is_lower and w.prob >= -15]\n",
    "    similarity = sorted(filtered_words, key=lambda w: word.similarity(w), reverse=True)\n",
    "    return [w.lower_ for w in similarity[:5]]\n",
    "\n",
    "for word in negative_words:\n",
    "    negative_words[word] = get_related(word)\n",
    "    \n",
    "for word in positive_words:\n",
    "    positive_words[word] = get_related(word)\n",
    "\n",
    "def create_new_sentences(tokens, target_words, c):\n",
    "    generated = pd.DataFrame(columns=['class', 'response_text', 'tokens'])\n",
    "    for word in target_words:\n",
    "        if word in tokens:\n",
    "            for syn in target_words[word][1:]:\n",
    "                new_tokens = tokens\n",
    "                \n",
    "                for i, n in enumerate(new_tokens):\n",
    "                    if n == word:\n",
    "                        #print(\"replacing %s with %s\" % (n, syn))\n",
    "                        new_tokens[i] = syn\n",
    "                #print(new_tokens)\n",
    "                string = \" \".join(str(x) for x in new_tokens)\n",
    "                temp = pd.DataFrame([[c, string, new_tokens]], columns=['class','response_text', 'tokens'])\n",
    "                generated = generated.append(temp)\n",
    "                new_list.append(temp)\n",
    "                #generated.append({'class':c ,\n",
    "                #                              'response_text':string, \n",
    "                #                              'tokens':new_tokens}, ignore_index=True )\n",
    "                #print(generated)\n",
    "    return generated\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    if row['class'] == 1:\n",
    "        data = data.append(create_new_sentences(row['tokens'], negative_words, 1))\n",
    "    else:\n",
    "        data = data.append(create_new_sentences(row['tokens'], positive_words, 0))\n",
    "        \n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_pickle(\"./data/newprocesseddata.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
